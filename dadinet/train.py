"""
Module for training and tuning MLPR with dadi-simulated data
"""
import pickle
import math
import numpy as np
from sklearn.neural_network import MLPRegressor
from mapie.regression import MapieRegressor
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingRandomSearchCV


def prep_data(data: dict, mapie=True):
    '''
    Helper method for outputing X and y from input data dict
    Input: data dict generated by generate_fs() method
    Output: X_input as a list of fs datasets
            y_label_unpack as a list of list, where each inner list
            is the label of one single demographic param if mapie,
            If mapie=False, y_label_unpack will be a list of one list,
            with the one inner list containing tuples of all dem params.
    '''

    # require dict to be ordered (Python 3.7+)
    X_input = [np.array(fs).flatten() for fs in data.values()]
    y_label = list(data.keys())

    # # longer implementation so as to not rely on data dict being ordered
    # X_input, y_label = [], []
    # for params, fs in data.items():
    #     X_input.append(np.array(fs).flatten())
    #     y_label.append(params)

    # parse labels into single list for each param (required for mapie)
    y_label_unpack = list(zip(*y_label)) if mapie else [y_label]

    return X_input, y_label_unpack


def tune(X_input, y_label, param_dist, max_iter=81, eta=3, cv=10):
    '''
    Method for searching over many MLPR hyperparameters
    with successive halving randomized search and hyperband
    Input:
        X_input: list of fs data sets from _prep_data()
        y_label: list of list of unpacked param labels from _prep_data()
        param_dist: param distribution dictionary specifying values of 
            hyperparams to search over
        max_iter: maximum iterations (lbfgs) or epochs (adam) allowed 
            per candidate, ideally a power of eta, e.g. 81 = 3^4
        eta: the halving parameter (factor)
        cv: k-fold of stratified cross validation
    Output: list of search results for each mlpr model
        Note: len(result_list) = len(y_label)
    '''

    s_max = int(math.log(max_iter)/math.log(eta))
    # number of unique executions of Successive Halving (minus one)

    result_list = []
    for param in y_label:
        mlpr = MLPRegressor()
        search_list = []
        # begin Hyperband outer loop
        for s in reversed(range(s_max+1)):
            n_iters = int(max_iter*eta**(-s))
            # begin Successive Halving inner loop implemented by sklearn
            search = HalvingRandomSearchCV(mlpr, param_dist,
                                           resource='max_iter', factor=eta,
                                           max_resources=max_iter,
                                           min_resources=n_iters, cv=cv,
                                           refit=False, n_jobs=-1)
            # note: resource is defined by max_iter rather than n_samples
            search.fit(X_input, param)
            search_list.append(search)
        result_list.append(search_list)

    return result_list


def report(results, file_name, n_top=3):
    '''Utility function to report best scores'''

    for i in range(1, n_top + 1):
        candidates = np.flatnonzero(results["rank_test_score"] == i)
        for candidate in candidates:
            print("Model with rank: {0}".format(i), file=file_name)
            print(
                "Mean validation score: {0:.3f} (std: {1:.3f})".format(
                    results["mean_test_score"][candidate],
                    results["std_test_score"][candidate],),
                file=file_name)
            print("Parameters: {0}".format(results["params"][candidate]),
                  file=file_name)


def get_best_specs(result_list):
    '''
    Helper method to extract best mlpr specs dict from the full result
    Input:
        result_list from tune() method
    Output:
        list of dict, where each dict is a mlpr_spec of the best mlpr
    '''

    mlpr_specs = []
    score_list = []
    # iterate through all successive halving searches to get best mlpr params
    for search_list in result_list:  # repeat for each mlpr model
        best_scores = [search.best_score_ for search in search_list]
        best_params = [search.best_params_ for search in search_list]
        best_score = max(best_scores)
        best_index = best_scores.index(best_score)
        mlpr_specs.append(best_params[best_index])
        score_list.append(best_score)

    return mlpr_specs, score_list


def train(X_input, y_label, mlpr_specs, mapie=True) -> list:
    '''
    Train one or multiple MLPR for one demographic model data set
    with either mapie or sklearn MLPR
    Input:
        X_input: list of fs data sets from _prep_data()
        y_label: list of list of unpacked param labels from _prep_data()
        mlpr_specs: list of dictionary of MLPR architecture specifications
            Note: len(y_label) = len(mlpr_spec) = len(mlpr_list)
        mapie: if False will use sklearn mlpr with multioutput option
        if True (default) will use mapie mlpr with single-output option
    Output:
        List of trained MLPR model(s)
        Single model if using sklearn, multiple models if using mapie
        Order of models in list is the same as order of dem parameters
    '''

    mlpr_list = []
    for param, mlpr_spec in zip(y_label, mlpr_specs):
        # param is a tuple of len(data) for one dem param if mapie
        # or a list of len(data) for all params tuples if sklearn

        # initiate one mlpr with mlpr_spec for each mlpr model
        mlpr = MLPRegressor()
        mlpr.set_params(**mlpr_spec)

        # use sklearn mlpr with multi-output option or mapie mlpr
        param_predictor = MapieRegressor(mlpr) if mapie else mlpr

        # train mlpr with input data
        param_predictor.fit(X_input, param)

        # save trained mlpr model
        mlpr_list.append(param_predictor)

    return mlpr_list
