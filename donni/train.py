"""
Module for training and tuning MVEnn with dadi-simulated data
"""
import logging
import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"  # FATAL
logging.getLogger("tensorflow").setLevel(logging.FATAL)
from tensorflow.python.framework.ops import disable_eager_execution
from multiprocessing import Pool

import numpy as np
import tensorflow as tf
from tensorflow import keras
from keras.models import Model
from keras.layers import Dense, Input, Dropout
from keras.callbacks import EarlyStopping
from tensorflow.python.framework.ops import disable_eager_execution
import keras.backend as K
import keras_tuner as kt


def prep_data(data: dict, mapie=True):
    """
    Helper method for outputing X and y from input data dict
    Input: data dict generated by generate_fs() method
    Output: X_input as a list of flattened fs datasets
            y_label_unpack as a list of list, where each inner list
            is the label of one single demographic param if mapie,
            If mapie=False, y_label_unpack will be a list of one list,
            with this one inner list containing tuples of all dem params.
    """

    # require dict to be ordered (Python 3.7+)
    X_input = [np.array(fs).flatten() for fs in data.values()]
    y_label = list(data.keys())

    # parse labels into single list for each param (required for mapie)
    y_label_unpack = list(zip(*y_label)) if mapie else [y_label]

    return np.array(X_input), y_label_unpack


def regression_nll_loss(sigma_sq, epsilon=1e-6):
    """Custom loss function to train both mean and variance"""

    def nll_loss(y_true, y_pred):
        return 0.5 * K.mean(
            K.log(sigma_sq + epsilon) + K.square(y_true - y_pred) / (sigma_sq + epsilon)
        )

    return nll_loss


def default_hyperparams():
    default_hp = kt.HyperParameters()
    default_hp.Choice(name="units_1", values=[32])
    default_hp.Choice(name="units_2", values=[16])
    default_hp.Boolean(name="dropout")
    default_hp.Choice(name="lr", values=[0.001])
    return default_hp


def tune(train_in, train_out, max_epochs=50):
    input_shape = train_in.shape[1]
    disable_eager_execution()

    def mvenn_base_model(units_1, units_2, dropout, lr):
        """Base network structure for all MVEnn"""
        inp = Input(shape=input_shape)
        x = Dense(units_1, activation="relu")(inp)
        if dropout:
            x = Dropout(0.2)(x)
        x = Dense(units_2, activation="relu")(x)
        mean = Dense(1, activation="linear")(x)
        var = Dense(1, activation="softplus")(x)  # softplus to ensure positive value

        train_model = Model(inp, mean)
        train_model.compile(
            loss=regression_nll_loss(var),
            optimizer=keras.optimizers.legacy.Adam(learning_rate=lr),
            metrics=[keras.metrics.RootMeanSquaredError()],
        )
        return train_model

    def model_builder(hp):
        """Hyperparam tuning"""
        # hyperparameters to be tuned
        units_1 = hp.Int("units_1", min_value=16, max_value=64, step=16)
        units_2 = hp.Int("units_2", min_value=4, max_value=16, step=4)
        dropout = hp.Boolean("dropout")
        lr = hp.Float("lr", min_value=1e-4, max_value=1e-2, sampling="log")

        # call existing model-building code with the hyperparameter values.
        tuned_train_model = mvenn_base_model(
            units_1=units_1, units_2=units_2, dropout=dropout, lr=lr
        )
        return tuned_train_model

    # instantiate the Hyperband tuner
    tuner = kt.Hyperband(
        model_builder,
        objective="val_loss",
        max_epochs=max_epochs,
        distribution_strategy=tf.distribute.MirroredStrategy(),
        directory="tuning_outdir",
        overwrite=True,
    )

    # Create a callback to stop training early after reaching a certain value for the val_loss
    stop_early = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=5)

    # Run the hyperparameter search
    tuner.search(
        train_in,
        train_out,
        epochs=30,
        validation_split=0.2,
        callbacks=[stop_early],
        verbose=0,
    )

    # tentative print tuner results
    tuner.results_summary()

    # Get the optimal hyperparameters
    best_hp = tuner.get_best_hyperparameters()[0]

    return best_hp


def train(best_hp, train_in, train_out, model_path, epochs=50):
    inp = Input(shape=train_in.shape[1])
    disable_eager_execution()
    x = Dense(best_hp.get("units_1"), activation="relu")(inp)
    dropout = best_hp.get("dropout")
    if dropout:
        x = Dropout(0.2)(x)
    x = Dense(best_hp.get("units_2"), activation="relu")(x)
    mean = Dense(1, activation="linear")(x)
    var = Dense(1, activation="softplus")(x)  # softplus to ensure positive value

    train_model = Model(inp, mean)
    train_model.compile(
        loss=regression_nll_loss(var),
        optimizer=keras.optimizers.legacy.Adam(learning_rate=best_hp.get("lr")),
        metrics=[keras.metrics.RootMeanSquaredError()],
    )
    pred_model = Model(inp, [mean, var])

    callback = EarlyStopping(monitor="val_loss", patience=5)
    train_model.fit(
        train_in,
        train_out,
        epochs=epochs,
        validation_split=0.2,
        callbacks=[callback],
        verbose=0,
    )

    pred_model.save(f"{model_path}")
