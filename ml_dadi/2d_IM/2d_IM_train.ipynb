{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import dadi\n",
    "import pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # this is the ml_dadi dir\n",
    "import data_manip, ml_models\n",
    "from data_manip import generating_data_parallel_log\n",
    "from ml_models import rfr_train, mlpr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of theta values to run scaling and add variance\n",
    "theta_list = [1,10000,1000,100] # order of increase variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid \n",
    "func = dadi.Demographics2D.IM\n",
    "ns = [20,20]\n",
    "pts_l = [40, 50, 60]\n",
    "logs = [False, True, True, False, False, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate parameter list for training:\n",
    "# Test a small number only here, increase when run on HPC\n",
    "train_params = [(s, nu1, nu2, T, m12, m21) for s in np.linspace(0.01, 0.99, 3)\n",
    "                            for nu1 in np.linspace(-2, 2, 3)\n",
    "                            for nu2 in np.linspace(-2, 2, 3)\n",
    "                            for T in np.linspace(0.1, 2, 3)\n",
    "                            for m12 in np.linspace(1, 10, 3)\n",
    "                            for m21 in np.linspace(1, 10, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  729\n",
      "Range of training params: (0.01, -2.0, -2.0, 0.1, 1.0, 1.0) to (0.99, 2.0, 2.0, 2.0, 10.0, 10.0)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# print training set info \n",
    "print('n_samples training: ', len(train_params))\n",
    "print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "print('Theta list:', theta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_dict = generating_data_parallel_log(train_params, \n",
    "                        theta_list, func, ns, pts_l, logs)\n",
    "pickle.dump(list_train_dict, open('data/train_data', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_train_dict = pickle.load(open('data/train_data','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RFR and save the list of trained RFR into pickle file\n",
    "list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict]\n",
    "pickle.dump(list_rfr, open('data/list_rfr', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet."
     ]
    }
   ],
   "source": [
    "# Train MLPR and save the list of trained MLPR into pickle file\n",
    "list_mlpr = [mlpr_train(train_dict, max_iter=1000) \n",
    "                for train_dict in list_train_dict]\n",
    "pickle.dump(list_mlpr, open('data/list_mlpr', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  100\n",
      "Range of testing params: (0.1206317193721481, 1.7458605843877417, 1.9254522844856357, 0.5573476249472297, 8.580966170018268, 6.989681408323595) to (0.895922808125844, 1.04527808685734, 0.6574315610822128, 1.6652855913282607, 6.7627242152791185, 4.189751563494047)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Datasets\n",
    "test_params = []\n",
    "for i in range(100):\n",
    "    s = random.random() * 0.98 + 0.01\n",
    "    nu1 = random.random() * 4 - 2\n",
    "    nu2 = random.random() * 4 - 2\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    m12 = random.random() * 9 + 1\n",
    "    m21 = random.random() * 9 + 1\n",
    "    params = (s, nu1, nu2, T, m12, m21)\n",
    "    test_params.append(params)\n",
    "# print testing set info \n",
    "print('n_samples testing: ', len(test_params))\n",
    "print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "print('Theta list:', theta_list)\n",
    "# Make a list of test data dictionaries, one dictionary for each theta case\n",
    "list_test_dict = generating_data_parallel_log(test_params, \n",
    "                    theta_list, func, ns, pts_l, logs)\n",
    "# Save testing set as a pickle file\n",
    "pickle.dump(list_test_dict, open('data/test_data', 'wb'), 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55a74aeb62132f0f3c0e21cb55598b2507f2ee10a4f6ed821cc7086dc4f1d0ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}