{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dadi\n",
    "import random\n",
    "import pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../..')) # this is the ml_dadi dir\n",
    "from data_manip import generating_data\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid\n",
    "func = dadi.Demographics1D.two_epoch\n",
    "ns = [20]\n",
    "pts_l = [40, 50, 60]\n",
    "# specify param in log scale\n",
    "logs = [True, False]\n",
    "theta_list = [1]\n",
    "test_params = []\n",
    "while len(test_params) < 1000:\n",
    "    log_nu = random.random() * 4 - 2\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    if T/10**log_nu <= 5:\n",
    "        test_params.append((log_nu, T))\n",
    "    \n",
    "list_test_dict = generating_data(test_params, theta_list, func, ns, pts_l, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(list_test_dict[0], open(f'data/gridsearch/train_data_1000', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_data_1d_2epoch_unnorm(n_samples, theta_list):\n",
    "#     # generate params\n",
    "#     params_dict = {}\n",
    "\n",
    "#     while len(params_dict) < n_samples:\n",
    "#         log_theta = random.random() * 3 + 2\n",
    "#         log_nu = random.random() * 4 - 2\n",
    "#         T = random.random() * 1.9 + 0.1\n",
    "#         params_dict[(log_nu, T)] = log_theta\n",
    "\n",
    "#     print(f'n_samples={len(params_dict)}')\n",
    "\n",
    "#     # designate demographic model, sample size, and extrapolation grid\n",
    "#     func = dadi.Demographics1D.two_epoch\n",
    "#     ns = [20]\n",
    "#     pts_l = [40, 50, 60]\n",
    "#     # theta_list = [1]\n",
    "#     # specify param in log scale\n",
    "#     logs = [True, False]\n",
    "\n",
    "#     # # generate data\n",
    "#     # data = generating_data(\n",
    "#     #     list(params_dict.keys()), theta_list, func, ns, pts_l, logs)\n",
    "\n",
    "#     # # relabel with non-dadi unit\n",
    "#     # # make new dict with new labels\n",
    "#     # data_dict = {}\n",
    "#     # # also need to times fs by unlog theta\n",
    "\n",
    "#     # # right now we have two dicts: params_dict and data[0]\n",
    "#     # # both have 1000 items and same log_nu, T tuple as keys\n",
    "#     # for key in list(params_dict.keys()):\n",
    "#     #     log_theta = params_dict[key]\n",
    "#     #     theta = 10**log_theta\n",
    "#     #     # make a new tuple key with new params and theta\n",
    "#     #     # this will be our new training label\n",
    "#     #     # new_key = key + tuple([log_theta])\n",
    "#     #     # print(new_key)\n",
    "\n",
    "#     #     # convert nu and T\n",
    "#     #     new_nu = math.log10(10**key[0] * theta / 4)\n",
    "#     #     new_T = key[1] * theta / 2\n",
    "#     #     # new_key = new_nu, new_T, log_theta\n",
    "#     #     new_key = new_nu, new_T\n",
    "\n",
    "#     #     # make new fs using unlog theta\n",
    "#     #     fs = data[0][key]\n",
    "#     #     scaled_fs = fs * theta\n",
    "\n",
    "#     #     # store into new data dict\n",
    "#     #     data_dict[new_key] = scaled_fs\n",
    "\n",
    "#     # return data_dict\n",
    "\n",
    "#     # generate data with dadi (parallelized function from data_manip)\n",
    "#     data = generating_data(list(params_dict.keys()),\n",
    "#                            theta_list, func, ns, pts_l, logs, sample=False)\n",
    "#     # data is a list of dict with one single element since theta_list=[1]\n",
    "\n",
    "#     # Make new params in non-dadi unit (nu and T become N and t)\n",
    "#     # make new dict to store new param labels and corresponding fs\n",
    "#     new_data = []\n",
    "\n",
    "#     # right now we have two dicts: params_dict and data[0],\n",
    "#     # (use data[0], not data: generating_data() created list of dicts)\n",
    "#     # both have n_samples items and the same (log_nu, T) tuple as keys\n",
    "\n",
    "#     # iterate through the keys to make new labels for data_dict\n",
    "#     for each in data:\n",
    "#         data_dict = {}\n",
    "#         for key in list(params_dict.keys()):\n",
    "#             # get theta in log and convert to non-log\n",
    "#             log_theta = params_dict[key]\n",
    "#             theta = 10**log_theta\n",
    "\n",
    "#             # convert nu and T to N and t using non-log theta\n",
    "#             nu = 10**key[0]  # key[0] is nu in log scale\n",
    "#             N = math.log10(nu * theta / 4)\n",
    "#             t = math.log10(key[1] * theta / 2)  # key[1] is T\n",
    "#             # both N and t will be in log scale\n",
    "#             new_key = N, t\n",
    "\n",
    "#             # make new fs using theta\n",
    "#             fs = each[key]\n",
    "#             scaled_fs = fs * theta\n",
    "\n",
    "#             # store labels and data into data_dict\n",
    "#             data_dict[new_key] = scaled_fs\n",
    "#         new_data.append(data_dict)\n",
    "\n",
    "#     return new_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=1000\n"
     ]
    }
   ],
   "source": [
    "# train_data = generate_data_1d_2epoch_unnorm(1000)\n",
    "# pickle.dump(train_data, open(f'data/1d_2epoch/train_data_1000', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid\n",
    "func = dadi.Demographics1D.two_epoch\n",
    "ns = [20]\n",
    "pts_l = [40, 50, 60]\n",
    "# specify param in log scale\n",
    "logs = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = []\n",
    "while len(test_params) < 100:\n",
    "    log_nu = random.random() * 4 - 2\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    test_params.append((log_nu, T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = [1e5, 1e4, 1e3, 1e2]\n",
    "# list_test_dict = generating_data(test_params, theta_list, func, ns, pts_l, logs, norm=False)\n",
    "list_test_dict = generating_data(test_params, theta_list, func, ns, pts_l, logs, norm=False, sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "# iterate through the keys to make new labels for data_dict\n",
    "for test_dict, theta in zip(list_test_dict, theta_list):\n",
    "    data_dict = {}\n",
    "    for key in list(test_dict.keys()):\n",
    "        # # get theta in log and convert to non-log\n",
    "        # log_theta = params_dict[key]\n",
    "        # theta = 10**log_theta\n",
    "\n",
    "        # convert nu and T to N and t using non-log theta\n",
    "        nu = 10**key[0]  # key[0] is nu in log scale\n",
    "        N = math.log10(nu * theta / 4)\n",
    "        t = math.log10(key[1] * theta / 2)  # key[1] is T\n",
    "        # both N and t will be in log scale\n",
    "        new_key = N, t\n",
    "\n",
    "        # make new fs using theta\n",
    "        # fs = each[key]\n",
    "        # scaled_fs = fs * theta\n",
    "\n",
    "        # store labels and data into data_dict\n",
    "        # data_dict[new_key] = scaled_fs\n",
    "        data_dict[new_key] = test_dict[key]\n",
    "    new_data.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(new_data, open(f'data/test_data_new_not_sampled', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193055.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check size\n",
    "list_test_dict[0][(-0.36087741527235373,\n",
    "  0.5906683957999392)].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193055.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[0][(4.037062593399684,4.4703137385307175)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19512.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[1][(3.0370625933996838,3.470313738530717)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1849.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[2][(2.0370625933996838,2.470313738530717)].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[3][(1.0370625933996838,1.4703137385307172)].sum()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "858cefacf0c5efe6fb099a66ddc33cd6b0e596bd331a46c92819ceda9cfe8c14"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
