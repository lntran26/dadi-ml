{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dadi\n",
    "import random\n",
    "import pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '../..')) # this is the ml_dadi dir\n",
    "from data_manip import generating_data\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_1d_2epoch_unnorm(n_samples):\n",
    "    # generate params\n",
    "    params_dict = {}\n",
    "\n",
    "    while len(params_dict) < n_samples:\n",
    "        log_theta = random.random() * 3 + 2\n",
    "        log_nu = random.random() * 4 - 2\n",
    "        T = random.random() * 1.9 + 0.1\n",
    "        params_dict[(log_nu,T)]= log_theta\n",
    "\n",
    "    print(f'n_samples={len(params_dict)}')\n",
    "\n",
    "    # designate demographic model, sample size, and extrapolation grid\n",
    "    func = dadi.Demographics1D.two_epoch\n",
    "    ns = [20]\n",
    "    pts_l = [40, 50, 60]\n",
    "    theta_list = [1]\n",
    "    # specify param in log scale\n",
    "    logs = [True, False]\n",
    "\n",
    "    # generate data\n",
    "    data = generating_data(\n",
    "        list(params_dict.keys()), theta_list, func, ns, pts_l, logs)\n",
    "\n",
    "    # relabel with non-dadi unit\n",
    "    # make new dict with new labels\n",
    "    data_dict = {}\n",
    "    # also need to times fs by unlog theta\n",
    "\n",
    "    # right now we have two dicts: params_dict and data[0]\n",
    "    # both have 1000 items and same log_nu, T tuple as keys\n",
    "    for key in list(params_dict.keys()):\n",
    "        log_theta = params_dict[key]\n",
    "        theta = 10**log_theta\n",
    "        # make a new tuple key with new params and theta\n",
    "        # this will be our new training label\n",
    "        # new_key = key + tuple([log_theta])\n",
    "        # print(new_key)\n",
    "\n",
    "        # convert nu and T\n",
    "        new_nu = math.log10(10**key[0] * theta / 4)\n",
    "        new_T = key[1] * theta / 2\n",
    "        # new_key = new_nu, new_T, log_theta\n",
    "        new_key = new_nu, new_T\n",
    "\n",
    "        # make new fs using unlog theta\n",
    "        fs = data[0][key]\n",
    "        scaled_fs = fs * theta\n",
    "\n",
    "        # store into new data dict\n",
    "        data_dict[new_key] = scaled_fs\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=1000\n"
     ]
    }
   ],
   "source": [
    "train_data = generate_data_1d_2epoch_unnorm(1000)\n",
    "pickle.dump(train_data, open(f'data/1d_2epoch/train_data_1000', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples=100\n"
     ]
    }
   ],
   "source": [
    "test_data = generate_data_1d_2epoch_unnorm(100)\n",
    "pickle.dump(test_data, open(f'data/1d_2epoch/test_data', 'wb'), 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cdc36f2de7190ea8b8383785aa4420ddc3efd64ef264b0251f8b4b81c783009f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
