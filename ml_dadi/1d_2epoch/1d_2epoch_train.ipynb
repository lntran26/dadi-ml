{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import dadi\n",
    "import pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # this is the ml_dadi dir\n",
    "import data_manip, ml_models\n",
    "from data_manip import generating_data_parallel, log_transform_data\n",
    "from ml_models import rfr_train, mlpr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_manip import generating_data_parallel_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of theta values to run scaling and add variance\n",
    "theta_list = [1,10000,1000,100] # order of increase variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid \n",
    "func = dadi.Demographics1D.two_epoch\n",
    "ns = [20]\n",
    "pts_l = [40, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate which param to be in log scale\n",
    "logs = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate parameter list for training: exclude params where T/nu > 5 version\n",
    "# train_params = [(nu,T) for nu in 10**np.linspace(-2, 2, 25)\n",
    "#                       for T in np.linspace(0.1, 2, 24) if T/nu <= 5]\n",
    "# # print training set info \n",
    "# print('n_samples training: ', len(train_params))\n",
    "# print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of training data dictionaries, one dictionary for each theta case\n",
    "# list_train_dict=generating_data_parallel(train_params,theta_list,func,ns,pts_l)\n",
    "# pickle.dump(list_train_dict, open('data/train_data_exclude', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  410\n",
      "Range of training params: (-1.6666666666666667, 0.1) to (2.0, 2.0)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# Generate parameter list for training: exclude params where T/nu > 5 version\n",
    "# using log version of the generating_data_parallel\n",
    "train_params = [(nu,T) for nu in np.linspace(-2, 2, 25)\n",
    "                      for T in np.linspace(0.1, 2, 24) if T/10**nu <= 5]\n",
    "# print training set info \n",
    "print('n_samples training: ', len(train_params))\n",
    "print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "print('Theta list:', theta_list)\n",
    "# Make a list of training data dictionaries, one dictionary for each theta case\n",
    "list_train_dict=generating_data_parallel_log(train_params, theta_list, func, ns, pts_l, logs)\n",
    "pickle.dump(list_train_dict, open('data/new_log/train_data_exclude_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate parameter list for training: full version (do not exclude params)\n",
    "# train_params = [(nu,T) for nu in 10**np.linspace(-2, 2, 21)\n",
    "#                         for T in np.linspace(0.1, 2, 20)]\n",
    "# # print training set info \n",
    "# print('n_samples training: ', len(train_params))\n",
    "# print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of training data dictionaries, one dictionary for each theta case\n",
    "# list_train_dict=generating_data_parallel(train_params,theta_list,func,ns,pts_l)\n",
    "# pickle.dump(list_train_dict, open('data/train_data_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load datasets for training\n",
    "# list_train_dict_ex = pickle.load(open('data/train_data_exclude','rb'))\n",
    "# list_train_dict_fl = pickle.load(open('data/train_data_full','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # log transform the 0th param, which is nu for the two epoch model\n",
    "# log_list_train_dict_ex = log_transform_data(list_train_dict_ex, [0])\n",
    "# log_list_train_dict_fl = log_transform_data(list_train_dict_fl, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new datasets for training\n",
    "list_train_dict_ex = pickle.load(open('data/new_log/train_data_exclude_log','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using log-transformed and T/nu>5 excluded datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in log_list_train_dict_ex]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_exclude_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using log-transformed and full range datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in log_list_train_dict_fl]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_full_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using original (not log) and T/nu>5 excluded datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict_ex]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_exclude', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using original (not log) and full range datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict_fl]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train MLPR using log-transformed and T/nu>5 excluded datasets\n",
    "# list_mlpr = [mlpr_train(train_dict, max_iter=500) \n",
    "#                 for train_dict in log_list_train_dict_ex]\n",
    "# pickle.dump(list_mlpr, open('data/list_mlpr_exclude_log_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train MLPR using log-transformed and full range datasets\n",
    "# list_mlpr = [mlpr_train(train_dict, max_iter=1000) \n",
    "#                 for train_dict in log_list_train_dict_fl]\n",
    "# pickle.dump(list_mlpr, open('data/list_mlpr_full_log_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train MLPR using log-transformed and T/nu>5 excluded datasets\n",
    "# list_mlpr = [mlpr_train(train_dict, solver='lbfgs', max_iter=3000) \n",
    "#                 for train_dict in log_list_train_dict_ex]\n",
    "# pickle.dump(list_mlpr, open('data/list_mlpr_exclude_log_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train MLPR using log-transformed and full range datasets, solver: lbfgs\n",
    "# list_mlpr = [mlpr_train(train_dict, solver='lbfgs', max_iter=5000) \n",
    "#                     for train_dict in log_list_train_dict_fl]\n",
    "# pickle.dump(list_mlpr, open('data/list_mlpr_full_log_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RFR using new log data and T/nu>5 excluded datasets\n",
    "list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict_ex]\n",
    "pickle.dump(list_rfr, open('data/new_log/list_rfr_exclude_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR using new log data, adam solver, and T/nu>5 excluded datasets\n",
    "list_mlpr = [mlpr_train(train_dict, max_iter=500) \n",
    "                for train_dict in list_train_dict_ex]\n",
    "pickle.dump(list_mlpr, open('data/new_log/list_mlpr_exclude_log_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR using new log data, lbfgs solver, and T/nu>5 excluded datasets\n",
    "list_mlpr = [mlpr_train(train_dict, solver='lbfgs', max_iter=5000) \n",
    "                for train_dict in list_train_dict_ex]\n",
    "pickle.dump(list_mlpr, open('data/new_log/list_mlpr_exclude_log_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Test Datasets: Full Range version\n",
    "# test_params = []\n",
    "# for i in range(100): # generate 100 randomly selected params in range\n",
    "#     nu = 10 ** (random.random() * 4 - 2)\n",
    "#     T = random.random() * 1.9 + 0.1\n",
    "#     params = (nu, T)\n",
    "#     test_params.append(params)\n",
    "# # print testing set info \n",
    "# print('n_samples testing: ', len(test_params))\n",
    "# print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of test data dictionaries, one dictionary for each theta case\n",
    "# list_test_dict=generating_data_parallel(test_params,theta_list,func,ns,pts_l)\n",
    "# # Save testing set as a pickle file\n",
    "# pickle.dump(list_test_dict, open('data/test_data_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generate Test Datasets: T/nu>5 Exclude version\n",
    "# test_params = []\n",
    "# for i in range(150): # higher than full range ver to get roughly 100 back\n",
    "# # generate random nu and T within the same range as training data range\n",
    "#     nu = 10 ** (random.random() * 4 - 2)\n",
    "#     T = random.random() * 1.9 + 0.1\n",
    "#     # exclude T/nu > 5\n",
    "#     if T/nu <= 5:\n",
    "#         params = (nu, T)\n",
    "#         test_params.append(params)\n",
    "# # print testing set info \n",
    "# print('n_samples testing: ', len(test_params))\n",
    "# print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of test data dictionaries, one dictionary for each theta case\n",
    "# list_test_dict=generating_data_parallel(test_params,theta_list,func,ns,pts_l)\n",
    "# # Save testing set as a pickle file\n",
    "# pickle.dump(list_test_dict, open('data/test_data_exclude', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  100\n",
      "Range of testing params: (-1.254557824244229, 0.13292083709916955) to (1.986104934244468, 1.2503572513980195)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Datasets: T/nu>5 Exclude version with new log data\n",
    "test_params = []\n",
    "while len(test_params) < 100: \n",
    "# generate random nu and T within the same range as training data range\n",
    "    nu = random.random() * 4 - 2\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    # exclude T/nu > 5\n",
    "    if T/10**nu <= 5:\n",
    "        params = (nu, T)\n",
    "        test_params.append(params)\n",
    "# print testing set info \n",
    "print('n_samples testing: ', len(test_params))\n",
    "print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "print('Theta list:', theta_list)\n",
    "# Make a list of test data dictionaries, one dictionary for each theta case\n",
    "list_test_dict=generating_data_parallel_log(test_params,theta_list,func,ns,pts_l, logs)\n",
    "# Save testing set as a pickle file\n",
    "pickle.dump(list_test_dict, open('data/new_log/test_data_exclude_log', 'wb'), 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55a74aeb62132f0f3c0e21cb55598b2507f2ee10a4f6ed821cc7086dc4f1d0ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}