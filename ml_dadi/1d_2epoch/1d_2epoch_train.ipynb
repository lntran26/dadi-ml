{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import dadi\n",
    "import pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # this is the ml_dadi dir\n",
    "import data_manip, ml_models\n",
    "from data_manip import generating_data_parallel, log_transform_data\n",
    "from ml_models import rfr_train, mlpr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of theta values to run scaling and add variance\n",
    "theta_list = [1,10000,1000,100] # order of increase variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid \n",
    "func = dadi.Demographics1D.two_epoch\n",
    "ns = [20]\n",
    "pts_l = [40, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  410\n",
      "Range of training params: (0.021544346900318832, 0.1) to (100.0, 2.0)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# # Generate parameter list for training: exclude params where T/nu > 5 version\n",
    "# train_params = [(nu,T) for nu in 10**np.linspace(-2, 2, 25)\n",
    "#                       for T in np.linspace(0.1, 2, 24) if T/nu <= 5]\n",
    "# # print training set info \n",
    "# print('n_samples training: ', len(train_params))\n",
    "# print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of training data dictionaries, one dictionary for each theta case\n",
    "# list_train_dict=generating_data_parallel(train_params,theta_list,func,ns,pts_l)\n",
    "# pickle.dump(list_train_dict, open('data/train_data_exclude', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  420\n",
      "Range of training params: (0.01, 0.1) to (100.0, 2.0)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# # Generate parameter list for training: full version (do not exclude params)\n",
    "# train_params = [(nu,T) for nu in 10**np.linspace(-2, 2, 21)\n",
    "#                         for T in np.linspace(0.1, 2, 20)]\n",
    "# # print training set info \n",
    "# print('n_samples training: ', len(train_params))\n",
    "# print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of training data dictionaries, one dictionary for each theta case\n",
    "# list_train_dict=generating_data_parallel(train_params,theta_list,func,ns,pts_l)\n",
    "# pickle.dump(list_train_dict, open('data/train_data_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets for training\n",
    "list_train_dict_ex = pickle.load(open('data/train_data_exclude','rb'))\n",
    "list_train_dict_fl = pickle.load(open('data/train_data_full','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log transform the 0th param, which is nu for the two epoch model\n",
    "log_list_train_dict_ex = log_transform_data(list_train_dict_ex, [0])\n",
    "log_list_train_dict_fl = log_transform_data(list_train_dict_fl, [0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using log-transformed and T/nu>5 excluded datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in log_list_train_dict_ex]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_exclude_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using log-transformed and full range datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in log_list_train_dict_fl]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_full_log', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using original (not log) and T/nu>5 excluded datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict_ex]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_exclude', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train RFR using original (not log) and full range datasets\n",
    "# list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict_fl]\n",
    "# pickle.dump(list_rfr, open('data/list_rfr_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR using log-transformed and T/nu>5 excluded datasets\n",
    "list_mlpr = [mlpr_train(train_dict, max_iter=500) \n",
    "                for train_dict in log_list_train_dict_ex]\n",
    "pickle.dump(list_mlpr, open('data/list_mlpr_exclude_log_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR using log-transformed and full range datasets\n",
    "list_mlpr = [mlpr_train(train_dict, max_iter=1000) \n",
    "                for train_dict in log_list_train_dict_fl]\n",
    "pickle.dump(list_mlpr, open('data/list_mlpr_full_log_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR using log-transformed and T/nu>5 excluded datasets\n",
    "list_mlpr = [mlpr_train(train_dict, solver='lbfgs', max_iter=3000) \n",
    "                for train_dict in log_list_train_dict_ex]\n",
    "pickle.dump(list_mlpr, open('data/list_mlpr_exclude_log_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.htmllbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html"
     ]
    }
   ],
   "source": [
    "# Train MLPR using log-transformed and full range datasets, solver: lbfgs\n",
    "list_mlpr = [mlpr_train(train_dict, solver='lbfgs', max_iter=5000) \n",
    "                    for train_dict in log_list_train_dict_fl]\n",
    "pickle.dump(list_mlpr, open('data/list_mlpr_full_log_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  100\n",
      "Range of testing params: (0.010345344152781892, 1.4250879148195665) to (96.65715565091608, 1.8103417713062555)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# # Generate Test Datasets: Full Range version\n",
    "# test_params = []\n",
    "# for i in range(100): # generate 100 randomly selected params in range\n",
    "#     nu = 10 ** (random.random() * 4 - 2)\n",
    "#     T = random.random() * 1.9 + 0.1\n",
    "#     params = (nu, T)\n",
    "#     test_params.append(params)\n",
    "# # print testing set info \n",
    "# print('n_samples testing: ', len(test_params))\n",
    "# print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of test data dictionaries, one dictionary for each theta case\n",
    "# list_test_dict=generating_data_parallel(test_params,theta_list,func,ns,pts_l)\n",
    "# # Save testing set as a pickle file\n",
    "# pickle.dump(list_test_dict, open('data/test_data_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  103\n",
      "Range of testing params: (0.07266535511395537, 0.1776327810852778) to (97.57511687691107, 1.8991357454084747)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# # Generate Test Datasets: T/nu>5 Exclude version\n",
    "# test_params = []\n",
    "# for i in range(150): # higher than full range ver to get roughly 100 back\n",
    "# # generate random nu and T within the same range as training data range\n",
    "#     nu = 10 ** (random.random() * 4 - 2)\n",
    "#     T = random.random() * 1.9 + 0.1\n",
    "#     # exclude T/nu > 5\n",
    "#     if T/nu <= 5:\n",
    "#         params = (nu, T)\n",
    "#         test_params.append(params)\n",
    "# # print testing set info \n",
    "# print('n_samples testing: ', len(test_params))\n",
    "# print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "# print('Theta list:', theta_list)\n",
    "# # Make a list of test data dictionaries, one dictionary for each theta case\n",
    "# list_test_dict=generating_data_parallel(test_params,theta_list,func,ns,pts_l)\n",
    "# # Save testing set as a pickle file\n",
    "# pickle.dump(list_test_dict, open('data/test_data_exclude', 'wb'), 2)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55a74aeb62132f0f3c0e21cb55598b2507f2ee10a4f6ed821cc7086dc4f1d0ba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}