{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dadi, random, pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # this is the ml_dadi dir\n",
    "import data_manip, ml_models\n",
    "from data_manip import generating_data\n",
    "from ml_models import rfr_train, mlpr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of theta values to run scaling and add variance\n",
    "theta_list_test = [1,10000,1000,100] # order of increase variance\n",
    "theta_list_train = [1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid \n",
    "func = dadi.Demographics1D.growth\n",
    "ns = [160] #############################sample size (values on axis), do what we did to theta list, try different values 10, 20, 40, 80, 160; no rfr\n",
    "pts_l = [40, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate which param to be in log scale\n",
    "logs = [True, False] #in this case, nu is on log scale, T is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  600\n",
      "Range of training params: (-2.0, 0.1) to (2.0, 2.0)\n",
      "Theta list: [1]\n"
     ]
    }
   ],
   "source": [
    " # Generate parameter list for training: exclude params where T/nu > 5 version\n",
    " # using log scale for nu\n",
    " # nu, T is the key to the data dictionary\n",
    "train_params = [(nu,T) for nu in np.linspace(-2, 2, 25) # (lower, upper, number of values); linspace equally spaces values\n",
    "                       for T in np.linspace(0.1, 2, 24)] #if T/10**nu <= 5] \n",
    "\n",
    " # print training set info \n",
    " print('n_samples training: ', len(train_params))\n",
    " print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    " print('Theta list:', theta_list_train)\n",
    " # Make a list of training data dictionaries, one dictionary for each theta case\n",
    " list_train_dict=generating_data(train_params, theta_list_train, func, ns, pts_l, logs) #generating_data is a function that uses the given info to generate an fs; the key to this fs is nu and T\n",
    " pickle.dump(list_train_dict, open('data/train_data_ns_160', 'wb'), 2) ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new datasets for training\n",
    "list_train_dict = pickle.load(open('data/train_data_ns_160','rb')) #########################only needed if we havent ran the previous block of code, i.e. if we haven't used list_train_dict before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR with adam solver (default)\n",
    "# Also test this one\n",
    "list_mlpr_adam = [mlpr_train(train_dict, max_iter=5000) # too high maxiter= runs too long, too low= non convergence\n",
    "                        for train_dict in list_train_dict]\n",
    "pickle.dump(list_mlpr_adam, open('data/list_mlpr_adam_ns_160', 'wb'), 2) #########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.htmllbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html"
     ]
    }
   ],
   "source": [
    "# Train MLPR with lbfgs solver\n",
    "#This will probably work the best\n",
    "# Need large max_iter and take longer to run but perform the best for two_epoch\n",
    "list_mlpr_lbfgs = [mlpr_train(train_dict, solver='lbfgs', max_iter=10000)\n",
    "                        for train_dict in list_train_dict]\n",
    "#mlpr_1 = mlpr_train(list_train_dict[2], solver='lbfgs', max_iter=10000)  # to test an individual theta value, uncomment this, comment out above\n",
    "pickle.dump(list_mlpr_lbfgs, open('data/list_mlpr_lbfgs_ns_10', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  100\n",
      "Range of testing params: (-1.1183195768851477, 0.2444060527496501) to (1.9467636371024035, 1.4501354314447392)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Datasets\n",
    "test_params = []\n",
    "while len(test_params) < 100: \n",
    "# generate random nu and T within the same range as training data range\n",
    "    nu = random.random() * 4 - 2 # nu in log scale\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    # exclude T/nu > 5\n",
    "    if T/10**nu <= 5: # only appends to list if this condition is satisfied; different from below\n",
    "        params = (nu, T)\n",
    "        test_params.append(params)\n",
    "# print testing set info \n",
    "print('n_samples testing: ', len(test_params))\n",
    "print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "print('Theta list:', theta_list_test)\n",
    "# Make a list of test data dictionaries, one dictionary for each theta case\n",
    "list_test_dict = generating_data(test_params, theta_list_test, func, ns, pts_l, logs)\n",
    "# Save testing set as a pickle file\n",
    "pickle.dump(list_test_dict, open('data/test_data_ns_160', 'wb'), 2) ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55a74aeb62132f0f3c0e21cb55598b2507f2ee10a4f6ed821cc7086dc4f1d0ba"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
