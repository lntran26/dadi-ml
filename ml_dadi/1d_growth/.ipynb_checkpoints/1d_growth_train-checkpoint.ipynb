{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dadi, random, pickle\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(os.getcwd(), '..')) # this is the ml_dadi dir\n",
    "import data_manip, ml_models\n",
    "from data_manip import generating_data\n",
    "from ml_models import rfr_train, mlpr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of theta values to run scaling and add variance\n",
    "theta_list = [1,10000,1000,100] # order of increase variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate demographic model, sample size, and extrapolation grid \n",
    "func = dadi.Demographics1D.growth\n",
    "ns = [20]\n",
    "pts_l = [40, 50, 60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# designate which param to be in log scale\n",
    "logs = [True, False] #in this case, nu is on log scale, T is not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples training:  410\n",
      "Range of training params: (-1.6666666666666667, 0.1) to (2.0, 2.0)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    " # Generate parameter list for training: exclude params where T/nu > 5 version\n",
    " # using log scale for nu\n",
    " # nu, T is the key to the data dictionary\n",
    " train_params = [(nu,T) for nu in np.linspace(-2, 2, 25) # (lower, upper, number of values); linspace equally spaces values\n",
    "                       for T in np.linspace(0.1, 2, 24) if T/10**nu <= 5] # \n",
    "\n",
    " # print training set info \n",
    " print('n_samples training: ', len(train_params))\n",
    " print('Range of training params:', min(train_params), 'to', max(train_params))\n",
    " print('Theta list:', theta_list)\n",
    " # Make a list of training data dictionaries, one dictionary for each theta case\n",
    " list_train_dict=generating_data(train_params, theta_list, func, ns, pts_l, logs) \n",
    " pickle.dump(list_train_dict, open('data/train_data', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load new datasets for training\n",
    "list_train_dict = pickle.load(open('data/train_data','rb')) #only needed if we havent ran the previous block of code, i.e. if we haven't used list_train_dict before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RFR \n",
    "# N/A as much to this model, but runs quick\n",
    "list_rfr = [rfr_train(train_dict) for train_dict in list_train_dict]\n",
    "pickle.dump(list_rfr, open('data/list_rfr', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR with adam solver (default)\n",
    "# Also test this one\n",
    "list_mlpr_adam = [mlpr_train(train_dict, max_iter=500) # too high maxiter= runs too long, too low= non convergence\n",
    "                        for train_dict in list_train_dict]\n",
    "pickle.dump(list_mlpr_adam, open('data/list_mlpr_adam', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLPR with lbfgs solver\n",
    "#This will probably work the best\n",
    "# Need large max_iter and take longer to run but perform the best for two_epoch\n",
    "list_mlpr_lbfgs = [mlpr_train(train_dict, solver='lbfgs', max_iter=5000)\n",
    "                        for train_dict in list_train_dict]\n",
    "pickle.dump(list_mlpr_lbfgs, open('data/list_mlpr_lbfgs', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples testing:  100\n",
      "Range of testing params: (-1.222463150850647, 0.26513143014436913) to (1.9193351005273405, 0.2635281256404377)\n",
      "Theta list: [1, 10000, 1000, 100]\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Datasets\n",
    "test_params = []\n",
    "while len(test_params) < 100: \n",
    "# generate random nu and T within the same range as training data range\n",
    "    nu = random.random() * 4 - 2 # nu in log scale\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    # exclude T/nu > 5\n",
    "    if T/10**nu <= 5: # only appends to list if this condition is satisfied; different from below\n",
    "        params = (nu, T)\n",
    "        test_params.append(params)\n",
    "# print testing set info \n",
    "print('n_samples testing: ', len(test_params))\n",
    "print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "print('Theta list:', theta_list)\n",
    "# Make a list of test data dictionaries, one dictionary for each theta case\n",
    "list_test_dict = generating_data(test_params, theta_list, func, ns, pts_l, logs)\n",
    "# Save testing set as a pickle file\n",
    "pickle.dump(list_test_dict, open('data/test_data', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Test Datasets Full Range\n",
    "test_params = []\n",
    "while len(test_params) < 100: #This gives 100 test values\n",
    "# generate random nu and T within the same range as training data range\n",
    "    nu = random.random() * 4 - 2 # nu in log scale #random.random is [0,1), this formula at the end ensures values are betweeen -2 and 2\n",
    "    T = random.random() * 1.9 + 0.1\n",
    "    # # exclude T/nu > 5\n",
    "    # if T/10**nu <= 5:\n",
    "    #     params = (nu, T)\n",
    "    #     test_params.append(params)\n",
    "    test_params.append((nu, T))\n",
    "\n",
    "# print testing set info \n",
    "print('n_samples testing: ', len(test_params))\n",
    "print('Range of testing params:', min(test_params), 'to', max(test_params))\n",
    "print('Theta list:', theta_list)\n",
    "# Make a list of test data dictionaries, one dictionary for each theta case\n",
    "list_test_dict = generating_data(test_params, theta_list, func, ns, pts_l, logs)\n",
    "# Save testing set as a pickle file\n",
    "pickle.dump(list_test_dict, open('data/new_func/test_data_full', 'wb'), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "55a74aeb62132f0f3c0e21cb55598b2507f2ee10a4f6ed821cc7086dc4f1d0ba"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
